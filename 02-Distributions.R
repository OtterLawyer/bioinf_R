# На пршлом занятии мы научились описывать наши данных с помощью разных метрик,
# таких как среднее, дисперсия, корреляция и т.д
# Здесь мы обсудим следующий этап, как распределены наши данные

# Автор: Алексей Замалутдинов
# коммент
#-------------------------------------------------------------------------

# Когда у нас несколько значений они так или иначе распределяются по своей встречаемости
# в наборе данных. На примере нашего датасета из домашнего задания, графически посмотрим
# как распределены данные. Для этого мы также вспомним (познакомимся) с ggplot2

library(dplyr)
library(ggplot2)
main_dir <- dirname(rstudioapi::getSourceEditorContext()$path) 
setwd(main_dir)

data <- read.csv("Life expectancy.csv")

# На примере одной переменной давайте взглянем, какие графики мы можем строить
# Этот, например, показывает сглаженную версию нашего распределения
ggplot(data=data, aes(x=Life_Expectancy_Men))+ 
  geom_density()
# Однако, чаще используют гистограммы, потому что их несколько легче интерпретировать
ggplot(data=data, aes(x=Life_Expectancy_Men, y=after_stat(density)))+ 
  geom_histogram()
# Обратите внимание, здесь указано количество, а не частота событий
# Давайте обратимся к справке, чтобы узнать как это можно исправить при необходимости
?geom_histogram

# Как бы вы описали распределение наших данных?
#-------------------------------------------------------------------------

# Перед тем как мы перейдём к распределениям, скажем пару слов о том, что зачем они нам нужны.
# Все данные, которые мы потенциально можем собрать или измерить - генеральная совокупность.
# Например, все деревья в плодовом саду, все рыбы в океане или всё человечество. 
# Как бы мы не пытались, мы вряд ли сможем измерить/взвесить/опросить всех представителей деревьев, рыб и людей
# Поэтому в подавляющем большинстве случаев мы работаем с выборками и достаточно часто эти выборки случайные. 

# То есть по случайным представителям генеральной совокупности мы пытаемся делать выводы о всех представителях
# В прошлый раз мы как раз считали различные показатели подобных выборок: среднее, разброс и тд.

# А теперь давайте подумаем, уникальны ли деревья/рыбы/люди как объекты исследования для статистики?

# К нашему счастью, нет!

# Именно для этого нам нужны распределения! 

# Если говорить строже, то мы говорим о распределении вероятностей, то есть
# насколько часто мы ожидаем встретить то или иное значение в наших данных.
# В целом их довольно много, но мы обсудим несколько наиболее важных
#
#-------------------------------------------------------------------------
# Начнём с дискретных распределений, где мы имеем дело с целыми числами значений
# Пример!
# Мы бросаем монетку 3 раза, как часто мы ожидаем встретить 3 орла? А 2?
#
# Если вы подзабыли что такое сочетания и вероятности, то рекомендую обратиться к курсу Яндекс.Практикума
# https://practicum.yandex.ru/math-foundations/
# Он бесплатный :)
#
dbinom(x=3, size=3, p=0.5) # ответ на первый вопрос
dbinom(x=2, size=3, p=0.5) # ответ на второй вопрос

# Посмотрим на график такого распределения
a <-  rbinom(n=100, size=3, prob=0.5)
a <- as.data.frame(a)
ggplot(a, aes(x=a, after_stat(density))) +
  geom_histogram(bins = 4)

# Распределение такого вида называется биномиальным. 
# У нас есть всего 2 исхода, успех/неуспех и их вероятность
# Заметим, что вероятности не точно соответствуют тому, 
# что мы насчитали пока отвечали на вопрос. Почему?


# Потому что это теоритическое распределение генеральной совокупности.  
# Если мы бесконечно много раз бросим монетку, то вероятности будут такими, 
# но в реальности таких вероятностей мы можем не добиться
#
# Как описать конкретное распределение, например, как в нашем случае с 3 бросками монетки?
# Для этого существуют параметры распределения. Что это в нашем случае?
#
# Для биномиального распределения есть два параметра: вероятность и число испытаний, 
# давайте поварьируем параметры и посмотрим на графики
#
a <-  rbinom(n=10000, size=10, prob=0.5)
a <- as.data.frame(a)
ggplot(a, aes(x=a, after_stat(density))) +
  geom_histogram(bins = 11)

a <-  rbinom(n=1000000, size=100, prob=0.05)
a <- as.data.frame(a)
ggplot(a, aes(x=a, after_stat(density))) +
  geom_histogram(bins = dim(unique(a))[1])

# Давайте расчитаем среднее и дисперсию нашего простого примера с монеткой и 3 бросками

mean_coin
var_coin

# Если обобщить то среднее равно n * p
# Дисперсия n * p * (p-1)
# Предположим, что известно, что 5% взрослых, принимающих определенное лекарство, испытывают негативные побочные эффекты. 
# Исходя из биномиального распределения мы можем рассчитать вероятность того, что более определенного
# числа пациентов в случайной выборке из 100 будут испытывать негативные побочные эффекты.
1 - pbinom(5, size=100, p=0.05)
# Посчитайте для 10 и 15 человек в выборке

#-------------------------------------------------------------------------
# Распределение Пуассона
# 
# Иногда нам нужно определить вероятность того, что 
# какое-то событие произойдёт N раз за промежуток времени.
# Например, число звонков или покупателей в магазине за час. 
# В отличие от биномиального распределения, мы не знаем
# какое максимальное количество событий возможно

# Единственный параметр распределения - лямбда (или мю), равный и среднему, и дисперсии
# Тогда функция вероятности выглядит так
# e^(-lambda)*lambda^k / k!

# Рассмотрим пример
# В колл-центр поступает в среднем 3 звонка в минуту. 
# Какова вероятность, что звонков будет больше 6?

b <-  rpois(n=100000, lambda = 3)
b <- as.data.frame(b)
ggplot(b, aes(x=b, after_stat(density))) +
  geom_histogram(bins = dim(unique(b))[1])
# Посчитаем вручную одну из вероятностей

# Теперь сверим с расчётами в R

dpois(6,3)

# Найдём ответ на наш вопрос

dpois(6,3) + dpois(5,3) + dpois(4,3) + dpois(3,3) + dpois(2,3) + dpois(1,3) + dpois(0,3)
ppois(6,3)

#-------------------------------------------------------------------------
# Перейдём к случаю, когда наши данные не являются целыми числами.
# И таких ситуаций довольно много, рост, вес, размер листа и тд и тп
# В таких случаях мы должны перейти к непрерывным распределениям
# Рассмотрим наиболее известное Нормальное распределение
# Сразу к примеру, потом формально опишем его
?LakeHuron

c <- as.data.frame(LakeHuron)
ggplot(c,aes(x)) +
  geom_density()
# 
# Значения случайной величины будут сгруппированы вокруг среднего значения, 
# и чем дальше от среднего значения, тем меньше вероятность того, что такое значение появится.
# 
# Почему оно "нормальное"?
# Всё просто, потому что значения в основном сгруппированы у нормы (то есть среднего)
# 
# Само распределение задаётся средним (то есть вокруг чего разбросаны наши данные) и
# стандартным отклонением (насколько широко разбросаны наши данные)
# Проще о том, как можно получить эту мудрёную формулу https://habr.com/ru/articles/730936/

# Теперь красивый пример
d <- rnorm(10000)
d <- as.data.frame(d)
ggplot(d,aes(d)) +
  geom_density() 
# В данном случае это стандартное нормальное распределение, среднее = 0, стандартное отклонение 1
# Давайте рассчитаем вероятности нескольких важных точек
pnorm(0) # по умолчанию параметры распределения соответствуют стандартному норм. распределению
# Теперь повторим для точек -1,-2,-3


# Дополним наш график найденныи значениями
ggplot(d,aes(d)) +
  geom_density() +
  geom_vline(aes(xintercept = 0)) +
  geom_vline(aes(xintercept = 1)) +
  geom_vline(aes(xintercept = 2)) +
  geom_vline(aes(xintercept = 3)) +
  annotate(geom = "text", x = 0.5, y = 0.5, label = "34,13%")+
  annotate(geom = "text", x = 1.5, y = 0.5, label = "13,59%")+
  annotate(geom = "text", x = 2.5, y = 0.5, label = "2,14%")+
  annotate(geom = "text", x = 3.5, y = 0.5, label = "0,13%")

# Поскольку "колокол" распределения симметричен, мы можем просуммировать значения
# ~68% наблюдений находятся между -1 и 1 (то есть 1 стандартное отклонение или сигма)
# ~95% наблюдений находятся между -2 и 2 (то есть 2 стандартных отклонения или сигма)
# ~99.7% наблюдений находятся между -3 и 3 (то есть 1 стандартных отклонений или сигма)

# Окей, с основами мы разобрались, теперь давайте попробуем что-то сделать на практике
# Приблизительно, наш пример с уровнем озера подходит под нормальное распределение
# Давайте найдём параметры возможного распределения
lake_mean <- 
lake_sd <- 

e <- rnorm(200000, mean = lake_mean, sd = lake_sd)
e <- as.data.frame(e)
ggplot()+
  geom_density(data=c,aes(x=x),colour="#ff0000") +
  geom_density(data=e,aes(x=e))
  
# Исходя из полученного распределения, какова вероятность, что уровень озера будет выше 585?

#-------------------------------------------------------------------------

# Снова вернёмся выборкам
# Создадим 200000 наблюдений из стандартного нормального распределения
# Затем возьмём оттуда несколько маленьких выборок
d <- rnorm(200000)
d <- as.data.frame(d)
d1 <- slice_sample(d,n=20)
d2 <- slice_sample(d,n=10)
d3 <- slice_sample(d,n=15)
d4 <- slice_sample(d,n=25)
d5 <- rbind(d1,d2,d3,d4) # сумма этих выборок
ggplot() +
  geom_density(data=d,aes(x=d))+
  geom_density(data=d1,aes(x=d),colour="#671c15")+
  geom_density(data=d2,aes(x=d),colour="#a83e1b")+
  geom_density(data=d3,aes(x=d),colour="#e8c238")+
  geom_density(data=d4,aes(x=d),colour="#2ca128")+
  geom_density(data=d5,aes(x=d),colour="#0000ff") 

# Мы знаем, что все эти выборки взяты из одного распределения
# Но каждая из них не воспроизводит "идеальное" (черным)
# Однако, если мы сложим их, то их сумма становится более похожа на нормальное (синим)
# Вот мы и плавно подошли к понятию центральной предельной теоремы (ЦПТ)
# 
# Если у нас есть бесконечная последовательность независимых 
# одинаково распределённых случайных величин, 
# то сумма таких распределений стремится к нормальному распределению
# 
# Однако, даже если величины распределены по-разному, но имеют одинаковый вес, 
# то итоговое распределение, всё равно стремится к нормальному

# Пример:
# Пусть число очков в дартсе зависит от:
# 1) Дрожжания руки
# 2) Ветра в комнате
# 3) Четкости зрения
# 4) Силы броска
# Каждая из этих величин наверняка имеет разные параметры распределения, 
# но итоговое распределение очков будет стремиться к нормальному распределению
